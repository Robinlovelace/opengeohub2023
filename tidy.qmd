---
format: html
title: Tidy geographic data
subtitle: "with sf, dplyr, ggplot2, geos and friends"
number-sections: true
# editor: 
#   render-on-save: true
bibliography: references.bib
---

These materials were created for the OpenGeoHub Summer School 2023.

They can be used with reference to the accompanying slides, available at [ogh23.robinlovelace.net/opengeohub2023](https://ogh23.robinlovelace.net/tidy-slides.html).

See the [parent repo](https://github.com/robinlovelace/opengeohub2023) and [session description in the agenda](https://pretalx.earthmonitor.org/opengeohub-summer-school-2023/talk/7JN3FV/) for context.

# Abstract {.unnumbered}

This lecture will provide an introduction to working with geographic data using R in a 'tidy' way.
It will focus on using the `sf` package to read, write, manipulate, and plot geographic data in combination with the `tidyverse` metapackage.
Why use the `sf` package with the `tidyverse`?
The lecture will outline some of the ideas underlying the `tidyverse` and how they can speed-up data analysis pipelines, while making data analysis code easier to read and write.
We will see how the following lines:

``` r
library(sf)
library(tidyverse)
```

can provide a foundation on which the many geographic data analysis problems can be solved.
The lecture will also cover on more recently developed packages that integrate with the `tidyverse` to a greater and lesser extent.
We will look at how the `geos` package, which provides a simple and high-performance interface to the GEOS library for performing geometric operations on geographic data, integrates with the `tidyverse`.
The `tidyverse` is not the right tool for every data analysis task and we touch on alternatives for working with raster data, with reference to the `terra` package, and alternative frameworks such as `data.table`.
Finally, we will also look at how the 'tidy' philosophy could be implemented in other programming languages, such as Python.

The focus throughout will be on practical skills and using packages effectively within the wider context of project management tools, integrated development environments (we recommend VS Code with appropriate extensions or RStudio), and version control systems.

# Introduction

## Learning objectives

By the end of the session, participants will be able to:

-   Read, write, manipulate, and plot geographic data using the `sf` package
-   Use the `tidyverse` metapackage to speed-up the writing of geographic data analysis pipelines
-   Use the `geos` package to perform geometric operations on geographic data
-   Understand the strengths and weaknesses of the `tidyverse` for geographic data analysis

## Prerequisites

We recommend you run the code in the practical session with a modern integrated development environment (IDE) such as

-   RStudio: an IDE focussed on data science and software development with R. See [posit.co](https://posit.co/download/rstudio-desktop/) for installation instructions.
-   VS Code: a general purpose, popular and future-proof IDE with support for R. See [github.com/REditorSupport/vscode-R](https://github.com/REditorSupport/vscode-R#getting-started) and [quarto.org](https://quarto.org/docs/get-started/) for installation instructions.

After you have installed a suitable IDE you will need to install R packages used in this tutorial.
You can install the packages we'll use with the following commands:

```{r}
#| message: false
#| warning: false
# Install remotes if not already installed
if (!requireNamespace("remotes")) {
    install.packages("remotes")
}

# The packages we'll use
pkgs = c(
    "sf",
    "tidyverse",
    "geos",
    "ggspatial",
    "spData"
)
```

```{r}
#| eval: false
remotes::install_cran(pkgs)
```

After running the above commands, you should be able to load the packages with the following command (we will load the packages individually in subsequent sections):

```{r}
#| eval: false
#| warning: false
sapply(pkgs, require, character.only = TRUE)
```

# An introduction to geographic data in the tidyverse

The `tidyverse` is a collection of packages that provides a unified set of functions for data science.
The name 'tidyverse' is a reference to the 'tidy data' concept, which means simple data that is in the form of one observation per row and one variable per column [@wickham2014].
The meaning has broadened to refer to a way of doing data analysis, that tends to make heavy use of tidyverse packages.
Load the `tidyverse` with the following command:

```{r}
library(tidyverse)
```

As shown in the output, the package loads 9 sub-packages.
In this tutorial we will focus on

-   `dplyr`, which provides convenient functions for manipulating data frames
-   `ggplot2`, which provides a powerful and flexible system for creating plots

A good way to understand it is to get started with a small dataset.
So let's load the `sf` package and the `spData` package, which contains the `world` dataset:

```{r}
library(sf)
library(spData)
```

After loading the packages run the following commands to create an object called countries, containing countries whose centroids are within 200km of the Polish border:

```{r}
names(world) # check we have the data
poland = world |>
    filter(name_long == "Poland")
world_centroids = world |>
    st_centroid()
country_centroids = world_centroids |>
  st_filter(
    poland,
    .predicate = st_is_within_distance,
    dist = 2e5
  )
countries = world |>
  filter(name_long %in% country_centroids$name_long)
countries_df = countries |>
  select(name_long, pop, area_km2) |>
  st_drop_geometry()
```

Don't worry about the syntax for now.
The important thing is that we now have a data frame with three columns, representing the name, population and area of four countries.
We can print out the contents of the data frame by typing its name (this is equivalent to `print(countries_df)`):

```{r}
countries_df
```

The output above shows information about each country in a tabular.
A feature of the tidyverse is that its default data frame class (the `tibble` which extends base R's `data.frame` as shown below) prints results in an informative and space-efficient way.

```{r}
class(countries_df)
```

`ggplot2` is dedicated plotting package that is loaded when you load the `tidyverse`.
It has native support for geographic objects, as shown in @fig-plotting-basics, which shows the output of `plot(countries)` next to the equivalent `ggplot2` code.

```{r}
#| label: fig-plotting-basics
#| fig.cap: "Plotting geographic data with base R (left) and ggplot2 (right)"
#| fig.subcap:
#|   - "Base R"
#|   - "ggplot2"
#| layout-ncol: 2
plot(countries)
countries |>
  ggplot() +
    geom_sf()
```

A characteristic feature of the tidyverse is the use of the pipe operator.
You can use R's new native pipe operator (`|>`), first available in R 4.1.0, or the magrittr pipe operator (`%>%`).

The pipes chain together functions, making it easier to read and write code.
It can be particularly useful when used in combination with RStudio's intellisense feature, which provides suggestions for column names as you type.
Try typing the following in RStudio and hitting Tab with the curso located between the brackets on the final line to see this in action.
It will allow you to select the variable you're interested in without quote marks, using a feature called non-standard evaluation (NSE) [@wickham2019].

```{r}
#| eval: false
countries_df |>
  filter()
```

## Reading and writing geographic data

You can read and write a wide range of vector geographic data with `sf`.
Save the `countries` object to a file called `countries.geojson` and inspect the result.

```{r}
sf::write_sf(countries, "countries.geojson", delete_dsn = TRUE)
```

You can read the file in again with `read_sf()` (which returns a 'tidyverse compliant' `tibble` data frame) or `st_read()`, as shown below.

```{r}
countries_new1 = sf::read_sf("countries.geojson")
countries_new2 = sf::st_read("countries.geojson")
```

For most purposes the two representations are the same, although the 'tibble' version's `print` outpout is slightly different.

```{r}
countries_new1 |>
  head(2)
countries_new2 |>
  head(2)
```

A nice function to explore the differences between the two objects is `waldo::compare()`.
It shows that, other than their classes, the two objects are identical:

```{r}
waldo::compare(countries_new1, countries_new2)
```

See the full list of file formats that you can read and write with `sf` with the following commands:

```{r}
drvs = sf::st_drivers() |>
  as_tibble()
head(drvs)
```

### Exercises

1.  Re-create the `country_centroids` object, using `world_centroids` and `poland` and inputs, but this time using base R syntax with the `[` operator.

    -   Bonus: use the `bench::mark()` function to compare the performance of the base R and tidyverse implementation
    -   Open question: Is this a good thing to benchmark? Why or why not?

```{r}
#| eval: false
#| echo: false
country_centroids2 = world_centroids[poland, , op = st_is_within_distance, dist = 2e5]
waldo::compare(country_centroids, country_centroids2)
#> ✔ No differences
res = bench::mark(
    base = world_centroids[poland, , op = st_is_within_distance, dist = 2e5],
    st_filter = world_centroids |>
  st_filter(poland, .predicate = st_is_within_distance, dist = 2e5)
)
res
#> # A tibble: 2 × 13
#>   expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time
#>   <bch:expr> <bch:tm> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>
#> 1 base         10.7ms 12.4ms      81.2     208KB     6.58    37     3      456ms
#> 2 st_filter      12ms 12.5ms      79.7     199KB     6.64    36     3      452ms
```

2.  Inspect the full list of drivers, e.g. with the command `View(drvs)`.
    -   Which formats are you likely to use and why?
    -   Bonus: take a look at [Chapter 8](https://r.geocompx.org/read-write) of Geocomputation with R for more on reading and writing geographic (including raster) data with R.

## Attribute operations with dplyr

**dplyr** is a large package with many functions for working with data frames.
The five key 'verbs' [described](https://dplyr.tidyverse.org/) as:

> -   `mutate()` adds new variables that are functions of existing variables
> -   `select()` picks variables based on their names.
> -   `filter()` picks cases based on their values.
> -   `summarise()` reduces multiple values down to a single summary.
> -   `arrange()` changes the ordering of the rows.

Let's take a brief look at each.

```{r}
countries_modified = countries |>
  mutate(pop_density = pop / area_km2) |>
  select(name_long, pop_density) |>
  filter(pop_density > 100) |>
  arrange(desc(pop_density))
countries_modified
```

The `summarise()` function is often used in combination with `group_by()`, e.g. as follows:

```{r}
countries_summarised = countries |>
  group_by(contains_a = str_detect(name_long, "a")) |>
  summarise(n = n(), mean_pop = mean(pop))
countries_summarised
```

The operation creates a new variable called `contains_a` that is `TRUE` if the country name contains an "a" and `FALSE` otherwise.
Perhaps more impressively, it also automatically updated the geometry column of the combined countries containing the letter "a", highlighting `dplyr`'s ability to work with geographic data represented as `sf` objects.

```{r}
#| label: fig-summarise
#| fig.cap: "Result of running dplyr group_by() and summarise() functions on countries data"
countries_summarised |>
  ggplot() +
    geom_sf(aes(fill = contains_a)) +
    geom_sf(data = countries, fill = NA, linetype = 3) 
```

### Exercises

1.  Create a new data frame called `countries_modified2` that contains the name, population and area of countries with a population density of more than 100 people per km2, sorted by area in descending order.
2.  Do the same with base R functions and the `[` operator.
    -   What are the pros and cons of each?
    -   Which do you prefer?

```{r}
#| eval: false
#| echo: false
# with dplyr:
countries_modified2 = countries |>
  mutate(pop_density = pop / area_km2) |>
  select(name_long, pop_density, area_km2) |>
  filter(pop_density > 100) |>
  arrange(desc(area_km2))
# with base R:
countries_base = countries[countries$pop / countries$area_km2 > 100, ]
countries_base = countries_base[countries_base$area_km2 > 100, c("name_long", "pop", "area_km2")]
countries_base = countries_base[order(countries_base$area_km2, decreasing = TRUE), ]

waldo::compare(countries_modified2$name_long, countries_base$name_long)
```

## Making maps with ggplot2

As shown above, `geom_sf()` works 'out of the box' with geographic data.
We can modify plotting commands to control outputs as showing in @fig-geom-sf-fill and generate publishable maps.

```{r}
#| label: fig-geom-sf-fill
#| fig.cap: "Map created with ggplot2, with fill color controlled by the pop_density variable and multiple layers."
library(ggspatial)
countries |>
  ggplot() +
    geom_sf(fill = "grey80", color = "black") +
    geom_sf(data = countries_modified, aes(fill = pop_density)) +
    scale_fill_viridis_c() +
    theme_minimal()
```

Map making is an iterative and time consuming process.
Iterate on the code above, e.g. by changing the color palette, adding a title, and adding a legend.

There are many add-ons to `ggplot2`.
`ggspatial` can be used to add a basemap to a plot with `annotation_map_tile()`, as illustrated in @fig-annotation-map-tile.

```{r}
#| message: false
#| label: fig-annotation-map-tile
#| fig.cap: "Map created with ggplot2, with a basemap added with annotation_map_tile()."
rosm::osm.types()
ggplot() +
  annotation_map_tile() +
  layer_spatial(countries_modified, aes(fill = pop_density),
                linewidth = 3, alpha = 0.3) +
  scale_fill_viridis_c()
```

### Exercises

1.  With reference to the documentation at [ggplot2.tidyverse.org/index.html](https://ggplot2.tidyverse.org/index.html), modify the code above to create a map with a title, legend and a different color palette.
2.  With reference to [paleolimbot.github.io/ggspatial/](https://paleolimbot.github.io/ggspatial/), add annotations including scale bar, north arrow and a text label to the map.
3.  Bonus: try map making with `tmap` and test out the interactive mode (set with `tmap_mode("interactive")`)."
4.  Bonus: try reproducing maps presented in [Chapter 9](https://r.geocompx.org/adv-map.html) of Geocomputation with R with `ggplot2` and `ggspatial`. Which mapping framework do you prefer and why?
5.  If you use raster data, take a look at the [`tidyterra`](https://dieghernan.github.io/tidyterra/) documentation.

# Data from OSM and data frames

We'll start this section by creating some data ourselves, representing 4 points of interest (pois) in relation to the OpenGeoHub Summer School 2023.

```{r}
#| eval: false
#| echo: false
fair_playce_poznan = stplanr::geo_code("FairPlayce, Poznan")
fair_playce_poznan
```

```{r}
poi_df = tribble(
  ~name, ~lon, ~lat,
  "Faculty",        16.9418, 52.4643,
  "Hotel ForZa",    16.9474, 52.4436,
  "Hotel Lechicka", 16.9308, 52.4437,
  "FairPlayce",     16.9497, 52.4604
)
poi_sf = sf::st_as_sf(poi_df, coords = c("lon", "lat"))
sf::st_crs(poi_sf) = "EPSG:4326"
```


```{r}
#| eval: false
# column: screen-inset-shaded
library(leaflet)
leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(
    lng = poi_df$lon,
    lat = poi_df$lat,
    popup = poi_df$name
  )
```

For comparison, we can create the same map with a single line of code with `{tmap}` (after loading and setting-up the package) as follows:

```{r}
library(tmap)
tmap_mode("view")
```

We will also check the version of `{tmap}` that we're using, in anticipation of a major update (to v4, expected later in 2023).

```{r}
#| eval: false
tm_shape(poi_sf) + tm_bubbles(popup.vars = "name")
```

The interactive maps below illustrate the outputs of the preceding map-making commands, highlighting the convenience of using `{tmap}`, which requires far fewer lines of code, although `{leaflet}` and other low-level map-making packages can offer more control.

::: {#fig-interactive layout-ncol="2"}

```{r}
#| column: screen-inset-shaded
#| layout-ncol: 2
#| echo: false
library(leaflet)
leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(
    lng = poi_df$lon,
    lat = poi_df$lat,
    popup = poi_df$name
  )

tm_shape(poi_sf) +
  tm_symbols(popup.vars = "name") +
  tm_scale_bar()
```

Interactive maps of 4 locations in Poznan made with 7 lines of code with `{leaflet}` (left), and 1 line of `{tmap}` code (right).

:::

Turn off the interactive mode as follows.

```{r}
tmap_mode("plot")
```

As a taster for the OSM workshop, let's read-in some data from OSM.
As our study area, we'll use a 500 m buffer around the convex hull of the Faculty and FairPlayce points.

```{r}
pois_buffer = poi_sf |>
  filter(str_detect(name, "Fair|Faculty")) |>
  st_union() |>
  st_convex_hull() |>
  st_buffer(dist = 500)
```

We'll save the output for future reference:

```{r}
pois_buffer_simple = pois_buffer |>
  st_simplify(dTolerance = 10)
plot(pois_buffer)
plot(pois_buffer_simple, add = TRUE, border = "blue")
sf::write_sf(pois_buffer_simple, "pois_buffer_simple.geojson", delete_dsn = TRUE)
```

```{r}
extra_tags = c("maxspeed", "foot", "bicycle")
```

```{r}
#| eval: false
lines = osmextract::oe_get(
  "Poznan",
  layer = "lines",
  extra_tags = extra_tags,
  boundary = pois_buffer,
  boundary_type = "clipsrc"
)
```

```{r}
#| include: false
lines = osmextract::oe_get(
  "Poznan",
  layer = "lines",
  extra_tags = extra_tags,
  boundary = pois_buffer,
  boundary_type = "clipsrc"
)
```

Filter out only the ones we need as follows:

```{r}
lines_highways = lines |>
  filter(!is.na(highway)) 
plot(lines$geometry)
table(lines$highway)
```

Let's add some polygons to the map, representing the buildings in the area.

```{r}
#| eval: false
polygons = osmextract::oe_get(
  "Poznan",
  layer = "multipolygons",
  boundary = pois_buffer,
  boundary_type = "clipsrc"
)
```

```{r}
#| include: false
polygons = osmextract::oe_get(
  "Poznan",
  layer = "multipolygons",
  boundary = pois_buffer,
  boundary_type = "clipsrc"
)
```

```{r}
buildings = polygons |>
  filter(!is.na(building))
polygons_geog = buildings |>
  filter(str_detect(name, "Geog"))
```

Plot the results as follows:

```{r}
#| label: fig-osm-pois
#| fig.cap: "OSM data from Poznan, Poland, with lines and polygons."
m_osm = tm_shape(buildings) +
  tm_fill("lightgrey") +
  tm_shape(lines_highways) +
  tm_lines() +
  tm_shape(polygons_geog) +
  tm_polygons(col = "red") 
```

You can export the `lines_highways` object to GeoJSON as follows:

```{r}
#| eval: false
sf::write_sf(lines_highways, "data/lines_highways.geojson")
```

## Loading a GPX file

Let's load a GPX file representing a route from the Faculty to FairPlayce.

```{r}
u_gpx = "https://www.openstreetmap.org/trace/9741677/data"
f_gpx = paste0(basename(u_gpx), ".gpx")
download.file(u_gpx, f_gpx)
sf::st_layers(f_gpx)
gpx = sf::read_sf(f_gpx, layer = "track_points")
```

We can divide the GPS points into n groups of equal length as follows:

```{r}
gpx_mutated = gpx |>
  mutate(minute = lubridate::round_date(time, "minute")) |>
  mutate(second = lubridate::round_date(time, "second")) 
summary(gpx_mutated$minute)
```

Let's create an animated map, as illustrated in @fig-gpx, with the following code:

```{r}
#| eval: false
# ?tmap_animation
m_faceted = m_osm +
  tm_shape(gpx_mutated[pois_buffer, ]) +
  tm_dots(size = 0.8, legend.show = FALSE) +
  tm_facets("second", free.coords = FALSE, ncol = 1, nrow = 1) +
  tm_scale_bar()
tmap_animation(m_faceted, delay = 2, filename = "gpx.gif")
```

<!---
 ![](gpx.gif) 
--->

::: {#fig-gpx}

![](https://user-images.githubusercontent.com/1825120/263552520-4079b188-f921-4c6f-9391-bfd7891a4f52.gif){width="100%"}

Animated map of a GPX file representing a route from central Poznan to FairPlayce.

:::

## Exercises

1. Create `sf` objects representing the following features in Poznan (hint: try using functions for geocoding such as `stplanr::geo_code()` or `tmaptools::geocode_OSM()`):
    -   The main train station
    -   The airport
2. Identify interesting features in the surrounding area from OSM and plot them in static and interactive maps.

# `tidyverse` + `geos`

The [`geos` package](https://paleolimbot.github.io/geos/) provides a simple and high-performance interface to the GEOS library for performing geometric operations on geographic data.
Load it as follows:

```{r}
library(geos)
```

`geos` works well with the `tidyverse` and `sf` packages, as shown below.

## Finding suitable coordinate reference systems

`geos` is designed to work with projected data, so we will reproject the `countries` object to a different CRS before proceeding.

```{r}
suitable_crs = crsuggest::suggest_crs(countries)
suitable_crs
```

We'll use the second of these, EPSG:2180, after [checking](https://github.com/Robinlovelace/opengeohub2023/issues/12) (the package's top suggestion is not always the most up-to-date or appropriate option).

```{r}
crs1 = paste0("EPSG:", suitable_crs$crs_code[2])
crs1
countries_projected = sf::st_transform(countries, crs = crs1)
```

```{r}
countries_geos = as_geos_geometry(sf::st_geometry(countries_projected))
countries_geos
```

The package only deals with geometries: the attribute data is removed when you convert an `sf` object to a `geos` object.
You can store `geos` objects in a data frame and still use `dplyr` functions to process them:

```{r}
countries_geos_df = bind_cols(countries_df, geos = countries_geos)
countries_summarised_df = countries_geos_df |>
  group_by(contains_a = str_detect(name_long, "a")) |>
  summarise(n = n(), mean_pop = mean(pop))
countries_summarised_df
```

Note: the `geos` column has gone!
This is because `geos` columns are not 'sticky' like `sf` columns.
Let's see how to get them back.

```{r}
#| layout-ncol: 2
countries_union1 = countries_geos |>
  geos_unary_union()
plot(countries_union1)
countries_union2 = countries_geos |>
  geos_make_collection() |>
  geos_unary_union()
plot(countries_union2)
```

However, you can add the union of the grouped columns as follows:

```{r}
countries_summarised_geos = countries_geos_df |>
  group_by(contains_a = str_detect(name_long, "a")) |>
  summarise(n = n(), mean_pop = mean(pop),
  geometry = geos_unary_union(geos_make_collection(geos)))
countries_summarised_geos
plot(countries_summarised_geos$geometry)
```

Convert back to an `sf` object as follows:

```{r}
countries_summarised_geos_sf = st_as_sf(countries_summarised_geos)
# waldo::compare(
#   countries_summarised,
#   countries_summarised_geos_sf
#   )
```

Aside from geometry names and minor differences in the geometries, the two objects are identical.
This raises the question: why use `geos` at all?
The answer can be found by following the exercises below.

## Exercises

1.  Benchmark the union operation in `geos` and `sf` with the `bench::mark()` function.
    -   Which is faster?
    -   Which is easier to use?
    -   Which do you prefer?

```{r}
#| eval: false
#| echo: false
bench::mark(check = FALSE,
  geos = countries_geos |>
    geos_make_collection() |>
    geos_unary_union(),
  geos_to_sf = countries_geos |>
    geos_make_collection() |>
    geos_unary_union() |>
    st_as_sf(),
  sf = countries_projected |>
    st_union()
)
```

# Worked example: data from Poznan

For many people 'learning by doing' is the most effective way to pick up new technical skills.
So let's work with some larger datasets.
Note: the input data and code below are adapted from Michael Dorman's tutorial on [Working with Spatial Data in Python](https://geobgu.xyz/presentations/p_2023_ogh).

## Getting and reading-in the data

```{bash}
#| eval: false
#| echo: false
# Zip the data folder:
zip -r data.zip data
gh release list
# Create release labelled data and upload the zip file:
gh release create data data.zip
```

To get the data for this session, download and unzip the [data.zip](https://github.com/Robinlovelace/opengeohub2023/releases/download/data/data.zip) file in the releases.
You can do that in R with the following commands:

```{r}
u = "https://github.com/Robinlovelace/opengeohub2023/releases/download/data/data.zip"
f = basename(u)
if (!dir.exists("data")) {
  download.file(u, f)
  unzip(f)
}
```

Check you have downloaded the files with the following command:

```{r}
list.files("data")[1:3]
```

## Vector data

Let's start by reading-in a dataset representing transport-related features around Poznan (note: you need to have downloaded and unzipped the `data.zip` file into your project or working directory for this to work):

```{r}
pol_all = sf::read_sf("./data/osm/gis_osm_transport_a_free_1.shp")
pol_all
```

Let's filter-out a feature that matches a particular character string:

```{r}
pol = pol_all |>
  filter(str_detect(name, "Port*.+Poz"))
```

Plot it, first with base R and then with `{ggplot2}`and `{tmap}`, resulting in maps shown below.

```{r}
#| layout-ncol: 3
#| column: screen-inset-shaded
plot(pol)
pol |>
  ggplot() +
  geom_sf()
tm_shape(pol) + tm_polygons()
```

We'll read-in a point layer from a CSV file as shown below.

```{r}
stops_raw = read_csv('data/gtfs/stops.txt')
stops_df = stops_raw |>
  select(-stop_code)
stops = st_as_sf(stops_df, coords = c("stop_lon", "stop_lat"), crs = "EPSG:4326")
```

### Buffers

The most widly used way to create buffers in R is with the function `st_buffer()` from the `sf` package.
Let's create buffers of 150 m around each of the points in the `poi_sf` dataset.
This is done in the following chunk, which first checks to see if the `s2` spherical geometry engine is set to run (it is by default).

```{r}
sf::sf_use_s2()
poi_buffers = st_buffer(poi_sf, dist = 150)
```

As described in [Chapter 7](https://r.geocompx.org/reproj-geo-data) or Geocomputation with R, `sf` 'knows that the world is round' and uses a spherical geometry engine to calculate distances for unprojected data.
This is a major advantage of `sf` over other packages for working with geographic data, such as `GeoPandas` in Python, which does not currently support spherical geometry operations (see [issue 2098 in the GeoPandas issue tracker for details](https://github.com/geopandas/geopandas/issues/2098)).

We can measure the area of the buffers with the following command:

```{r}
areas = st_area(poi_buffers)
```

A nice feature of `sf` is that it returns the area in square meters, even though the input data is in degrees.
`sf` uses the `units` package behind the scenes to convert between units, meaning you can convert the output to different units, as shown below.

```{r}
areas |>
  units::set_units(ha)
```

Sometimes it's useful to drop the `units` class, which can be done with the `units::drop_units()` function, as shown below.

```{r}
areas |>
  units::set_units(ha) |>
  units::drop_units() |>
  round()
```

### Spatial subsetting

Note: this section is adapted from [Section 2.12](https://geobgu.xyz/presentations/p_2023_ogh/01-vector.html#sec-subsetting-by-location) of [Working with Spatial Data in Python](https://geobgu.xyz/presentations/p_2023_ogh).
Let's find the bus stops that are within 150 m of the `poi_sf` points.

```{r}
stops_nearby = stops[poi_buffers, ]
stops_nearby
```

### Spatial joins

Spatial joins can be performed with the `st_join()` function as follows:

```{r}
pois_joined = st_join(poi_buffers, stops)
pois_joined
```

### Exercises

1.  Create a static map of the stops in Poznan, using the `stops` object created above, with a mapping package of your preference. Set the colour of each stop by `zone_id`.
    - Bonus: also create an interactive map.
2. **Advanced**: Reproduce the results presented above by following the Python code at [geobgu.xyz/presentations/p_2023_ogh/01-vector.html](https://geobgu.xyz/presentations/p_2023_ogh/01-vector.html).
    - Which language do you prefer for the types of task presented here and why?

## Raster data example

Building on the introduction to [raster data with Python](https://geobgu.xyz/presentations/p_2023_ogh/02-raster.html), this section introduces raster data with the `{terra}` package.

Load it as follows:

```{r}
library(terra)
```

Read-in and plot a single raster layer with the following command:

```{r}
#| label: fig-terra-plot
#| fig.cap: "Plotting a single raster layer with terra"
src = rast('data/hls/HLS.S30.T33UXU.2022200T095559.v2.0.B02.tiff')
terra::plot(src, col = gray.colors(10))
```

We will translate the following Python code to R:

```python
files = glob.glob('data/hls/*.tiff')
```

```{r}
files = list.files("data/hls", pattern = "tiff", full.names = TRUE)
files
```

```{r}
r = rast(files)
r
summary(r)
```

We can plot the result as follows:

```{r}
#| label: fig-terra-plot-basic
#| fig.cap: "Output of `plot(r)`, showing the four bands of the raster layer"
plot(r)
```

As shown, the result is an object with Blue, Green, Red and NIR bands, in that order.
We can select only Red, Green, Blue bands, in that order, as follows:

```{r}
r_rgb = r[[c("Red", "Green", "Blue")]]
```

If you try plotting the result with `plotRGB(r)`, you will get an error.
You can use the `stretch` argument of the function to stretch the values and avoid errors caused by outliers.

```{r}
#| label: fig-terra-plot-rgb
#| fig.cap: "Plotting a RGB raster layer with terra"
plotRGB(r, stretch = "lin")
```

We can also remove outliers with the `stretch()` or `clamp()` functions or manually, as shown below:

```{r}
#| eval: false
# r_clamp = clamp(r, 0, 4000)
r_stretch = stretch(r_rgb, minq = 0.001, maxq = 0.999)
top_01pct = quantile(values(r_rgb), probs = 0.999, na.rm = TRUE)
bottom_01pct = quantile(values(r_rgb), probs = 0.001, na.rm = TRUE)
r_to_plot = r_rgb
r_to_plot[r_rgb > top_01pct] = top_01pct
r_to_plot[r_rgb < bottom_01pct] = bottom_01pct
```

```{r}
#| include: false
# r_clamp = clamp(r, 0, 4000)
r_stretch = stretch(r_rgb, minq = 0.001, maxq = 0.999)
top_01pct = quantile(values(r_rgb), probs = 0.999, na.rm = TRUE)
bottom_01pct = quantile(values(r_rgb), probs = 0.001, na.rm = TRUE)
r_to_plot = r_rgb
r_to_plot[r_rgb > top_01pct] = top_01pct
r_to_plot[r_rgb < bottom_01pct] = bottom_01pct
```

::: {#fig-terra-plot-rgb-clamp}

```{r}
#| echo: false
#| layout-ncol: 2
plotRGB(r_stretch)
plotRGB(r_to_plot)
```

Plotting a RGB raster layer with terra, with outliers handled with `stretch()` (left) and manually (right)

:::

Save the combined raster as follows:

```{r}
#| eval: false
# write the r file:
writeRaster(r, "data/hls/combined.tif", overwrite = TRUE)
writeRaster(r_to_plot, "data/hls/r_to_plot.tif", overwrite = TRUE)
```

### Masking and cropping

We can mask the raster with the `pol` polygon object as follows:

```{r}
pol_projected = sf::st_transform(pol, crs = crs(r))
r_masked = mask(r, pol_projected)
summary(r_masked)
```

As shown in the summary of the result, the majority of the values are now NA.
That's how masking works: it sets all values outside the polygon to NA.
We can crop the raster to a 500 m buffer around the polygon as follows:

```{r}
r_cropped = crop(r, sf::st_buffer(pol_projected, dist = 500))
```

Let's plot the result, illustrated in @fig-terra-plot-rgb-cropped.

```{r}
#| label: fig-terra-plot-rgb-cropped
#| fig.cap: "Result of plotting a cropped RGB raster layer with the tmap package"
tm_shape(stretch(r_cropped[[c("Red", "Green", "Blue")]], minq = 0.001, maxq= 0.98)) +
  tm_rgb() +
  tm_shape(pol_projected) +
  tm_borders(col = "white", lty = 3) +
  tm_fill(col = "red", alpha = 0.1) +
  tm_scale_bar(bg.color = "white", bg.alpha = 0.6, text.size = 0.8) 
```

### Exercises

1. Experiment with arguments passed to `clamp()`, `stretch()` and `plotRGB()` to see how they affect the output.
2. Try plotting the raster in another program like QGIS, which looks better?
3. **Advanced**: Reproduce the results presented above by following the Python code at [geobgu.xyz/presentations/p_2023_ogh/02-raster.html](https://geobgu.xyz/presentations/p_2023_ogh/02-raster.html).
    - Which language do you prefer for the types of task presented here and why?
4. **Advanced**: Try plotting the raster with `{ggplot2}` and `{tmap}`.
    - Which do you prefer and why?

# Other tidy packages

There are many packages that integrate with the `tidyverse`, or which are inspired by it.
Options include:

-   [`tidyterra`](https://dieghernan.github.io/tidyterra/) for working with raster data
-   [DuckDB](https://duckdb.org/docs/api/r.html) is a high-performance in-memory database that can be used with `dplyr` that is mature, well documented, and has a [spatial extension](https://duckdb.org/docs/extensions/spatial)
-   [dtplyr](https://github.com/tidyverse/dtplyr/), a `dplyr`-compliant interface to [`data.table`](https://rdatatable.gitlab.io/data.table/) for working with large datasets
-   The `collapse` package, which supports `tidyverse` syntax and which has some [integration with `sf`](https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html)
-   [`tidypolars`](https://github.com/etiennebacher/tidypolars), an R interface to the [`polars`](https://github.com/pola-rs/polars) Rust crate (note: there is also a popular [Python interface](https://pola-rs.github.io/polars-book/user-guide/installation/)
-   [The geoarrow spec](https://github.com/geoarrow) and nascent implementations, none of which are yet ready for production use

How to assess which to use?
My advice would be to focus on solving the problem, rather than the tools, in which case the `tidyverse` + `sf` + `terra` or `stars` should be more than adequate for most purposes.
There can be major time savings made by sticking to established tools that are well documented and which have a large user base.

Developer time is more valuable than computer time, so a major factor in deciding which tool to use should be how reliable it is.
Time spent debugging issues can be a major time sink, especially for new packages that are not yet well tested or documented.
And for most purposes, the tools outlined in the previous sections are more than adequate, solving perhaps 90% of the problems you're likely to encounter.

There are times when performance is important but as the saying goes, premature optimisation is the root of all evil, so don't worry about performance until you have a working solution.

If you *do* need to wait longer than is feasible for your code to run, it is worth trying to identify the bottleneck (is there are a solution involving changing input datasets or workflows?) before having a look at alternatives.
Established benchmarks for *non-spatial data* are https://duckdblabs.github.io/db-benchmark/ and https://www.pola.rs/benchmarks.html .

This section is offered more as a place to experiment than any concrete advice.
My concrete advice is focus on the problem and solving it with the software is most appropriate for your use case.

## `tidypolars`

If you want to give this package a spin, run the following command:

```{r}
#| eval: false
install.packages(
  'tidypolars', 
  repos = c('https://etiennebacher.r-universe.dev/bin/linux/jammy/4.3', getOption("repos"))
)
```

## `rsgeo`

A work in progress is the [`rsgeo` package](https://github.com/JosiahParry/rsgeo), which aims to provide an seamless interface between R and the `geo` Rust crate.
This could open the possiblity of calling other high-performance Rust libraries from R, although the package is at an early stage of development and probably not ready for production use.

We can check the installation works as follows:

```{r}
#| eval: false
install.packages('rsgeo', repos = c('https://josiahparry.r-universe.dev', 'https://cloud.r-project.org'))
```

```{r}
#| eval: false
library(rsgeo)
countries_rs  = as_rsgeo(sf::st_geometry(countries_projected))
countries_rs
bench::mark(check = FALSE,
  sf = sf::st_union(countries_projected),
  geos = geos::geos_make_collection(geos::geos_unary_union(countries_geos)),
  rsgeo = rsgeo::union_geoms(countries_rs)
)
```

## Arrow

```{r}
#| eval: false
library(arrow)

# write countries_projected to parquet file:
write_parquet(countries_projected, "data/countries_projected.parquet") # Fails

```


# Further reading

For more on raster data see https://kadyb.github.io/OGH2023/Clustering.html



The following free and open access resources provide a strong foundation for further learning in this space.

-   @wickham2023, hosted at <https://r4ds.hadley.nz/>, is a detailed introduction to the tidyverse

-   @lovelace2019 provides a wide ranging yet beginner-friendly introduction to using R for geographic analysis

-   @pebesma2023, a textbook on spatial data science by the developers of `sf` and other core 'rspatial' packages

-   Any other suggestions welcome, there's much more out there!