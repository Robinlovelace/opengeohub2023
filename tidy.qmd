---
format: html
title: Tidy geographic data
subtitle: "with sf, dplyr, ggplot2, geos and friends"
number-sections: true
# editor: 
#   render-on-save: true
bibliography: references.bib
---

These materials were created for the OpenGeoHub Summer School 2023.

They can be used with reference to the accompanying slides, available at [ogh23.robinlovelace.net/opengeohub2023](https://ogh23.robinlovelace.net/tidy-slides.html).

See the [parent repo](https://github.com/robinlovelace/opengeohub2023) and [session description in the agenda](https://pretalx.earthmonitor.org/opengeohub-summer-school-2023/talk/7JN3FV/) for context.

# Abstract {.unnumbered}

This lecture will provide an introduction to working with geographic data using R in a 'tidy' way. It will focus on using the `sf` package to read, write, manipulate, and plot geographic data in combination with the `tidyverse` metapackage. Why use the `sf` package with the `tidyverse`? The lecture will outline some of the ideas underlying the `tidyverse` and how they can speed-up data analysis pipelines, while making data analysis code easier to read and write. We will see how the following lines:

``` r
library(sf)
library(tidyverse)
```

can provide a foundation on which the many geographic data analysis problems can be solved. The lecture will also cover on more recently developed packages that integrate with the `tidyverse` to a greater and lesser extent. We will look at how the `geos` package, which provides a simple and high-performance interface to the GEOS library for performing geometric operations on geographic data, integrates with the `tidyverse`. The `tidyverse` is not the right tool for every data analysis task and we touch on alternatives for working with raster data, with reference to the `terra` package, and alternative frameworks such as `data.table`. Finally, we will also look at how the 'tidy' philosophy could be implemented in other programming languages, such as Python.

The focus throughout will be on practical skills and using packages effectively within the wider context of project management tools, integrated development environments (we recommend VS Code with appropriate extensions or RStudio), and version control systems.

# Introduction

## Learning objectives

By the end of the session, participants will be able to:

-   Read, write, manipulate, and plot geographic data using the `sf` package
-   Use the `tidyverse` metapackage to speed-up the writing of geographic data analysis pipelines
-   Use the `geos` package to perform geometric operations on geographic data
-   Understand the strengths and weaknesses of the `tidyverse` for geographic data analysis

## Prerequisites

We recommend you run the code in the practical session with a modern integrated development environment (IDE) such as

-   RStudio: an IDE focussed on data science and software development with R. See [posit.co](https://posit.co/download/rstudio-desktop/) for installation instructions.
-   VS Code: a general purpose, popular and future-proof IDE with support for R. See [github.com/REditorSupport/vscode-R](https://github.com/REditorSupport/vscode-R#getting-started) and [quarto.org](https://quarto.org/docs/get-started/) for installation instructions.

After you have installed a suitable IDE you will need to install R packages used in this tutorial. You can install the packages we'll use with the following commands:

```{r}
#| message: false
#| warning: false
# Install remotes if not already installed
if (!requireNamespace("remotes")) {
    install.packages("remotes")
}

# The packages we'll use
pkgs = c(
    "sf",
    "tidyverse",
    "geos",
    "ggspatial",
    "spData"
)
```

```{r}
#| eval: false
remotes::install_cran(pkgs)
```

After running the above commands, you should be able to load the packages with the following command (we will load the packages individually in subsequent sections):

```{r}
#| eval: false
#| warning: false
sapply(pkgs, require, character.only = TRUE)
```

## An introduction to tidy geographic data

The `tidyverse` is a collection of packages that provides a unified set of functions for data science. The name 'tidyverse' is a reference to the 'tidy data' concept, which means simple data that is in the form of one observation per row and one variable per column [@wickham2014]. The meaning has broadened to refer to a way of doing data analysis, that tends to make heavy use of tidyverse packages. Load the `tidyverse` with the following command:

```{r}
library(tidyverse)
```

As shown in the output, the package loads 9 sub-packages. In this tutorial we will focus on

-   `dplyr`, which provides convenient functions for manipulating data frames
-   `ggplot2`, which provides a powerful and flexible system for creating plots

A good way to understand it is to get started with a small dataset. So let's load the `sf` package and the `spData` package, which contains the `world` dataset:

```{r}
library(sf)
library(spData)
```

After loading the packages run the following commands to create an object called countries, containing countries whose centroids are within 200km of the Polish border:

```{r}
names(world) # check we have the data
poland = world |>
    filter(name_long == "Poland")
world_centroids = world |>
    st_centroid()
country_centroids = world_centroids |>
  st_filter(poland, .predicate = st_is_within_distance, dist = 2e5)
countries = world |>
  filter(name_long %in% country_centroids$name_long)
countries_df = countries |>
  select(name_long, pop, area_km2) |>
  st_drop_geometry()
```

Don't worry about the syntax for now. The important thing is that we now have a data frame with three columns, representing the name, population and area of four countries. We can print out the contents of the data frame by typing its name (this is equivalent to `print(countries_df)`):

```{r}
countries_df
```

The output above shows information about each country in a tabular. A feature of the tidyverse is that its default data frame class (the `tibble` which extends base R's `data.frame` as shown below) prints results in an informative and space-efficient way.

```{r}
class(countries_df)
```

`ggplot2` is dedicated plotting package that is loaded when you load the `tidyverse`. It has native support for geographic objects, as shown in the figure below which shows the output of `plot(countries)` next to the equivalent `ggplot2` code.

```{r}
#| layout-ncol: 2
plot(countries)
countries |>
  ggplot() +
    geom_sf()
```

## Pipes

A characteristic feature of the tidyverse is the use of the pipe operator. You can use R's new native pipe operator (`|>`), first available in R 4.1.0, or the magrittr pipe operator (`%>%`). The pipe operator is used to chain together functions, making it easier to read and write code. It can be particularly useful when used in combination with RStudio's intellisense feature, which provides suggestions for column names as you type. Try typing the following in RStudio and hitting Tab with the curso located between the brackets on the final line to see this in action. It will allow you to select the variable you're interested in without quote marks, using a feature called non-standard evaluation (NSE) [@wickham2019].

```{r}
#| eval: false
countries_df |>
  filter()
```

## Reading and writing geographic data

You can read and write a wide range of vector geographic data with `sf`. Save the `countries` object to a file called `countries.geojson` and inspect the result.

```{r}
sf::write_sf(countries, "countries.geojson", delete_dsn = TRUE)
```

You can read the file in again with `read_sf()` (which returns a 'tidyverse compliant' `tibble` data frame) or `st_read()`, as shown below.

```{r}
countries_new1 = sf::read_sf("countries.geojson")
countries_new2 = sf::st_read("countries.geojson")
```

For most purposes the two representations are the same, although the 'tibble' version's `print` outpout is slightly different.

```{r}
countries_new1 |>
  head(2)
countries_new2 |>
  head(2)
```

A nice function to explore the differences between the two objects is `waldo::compare()`.
It shows that, other than their classes, the two objects are identical:

```{r}
waldo::compare(countries_new1, countries_new2)
```

See the full list of file formats that you can read and write with `sf` with the following commands:

```{r}
drvs = sf::st_drivers() |>
  as_tibble()
head(drvs)
```

## Exercises

1.  Re-create the `country_centroids` object, using `world_centroids` and `poland` and inputs, but this time using base R syntax with the `[` operator.

    -   Bonus: use the `bench::mark()` function to compare the performance of the base R and tidyverse implementation
    -   Open question: Is this a good thing to benchmark? Why or why not?

```{r}
#| eval: false
#| echo: false
country_centroids2 = world_centroids[poland, , op = st_is_within_distance, dist = 2e5]
waldo::compare(country_centroids, country_centroids2)
#> ✔ No differences
res = bench::mark(
    base = world_centroids[poland, , op = st_is_within_distance, dist = 2e5],
    st_filter = world_centroids |>
  st_filter(poland, .predicate = st_is_within_distance, dist = 2e5)
)
res
#> # A tibble: 2 × 13
#>   expression      min median `itr/sec` mem_alloc `gc/sec` n_itr  n_gc total_time
#>   <bch:expr> <bch:tm> <bch:>     <dbl> <bch:byt>    <dbl> <int> <dbl>   <bch:tm>
#> 1 base         10.7ms 12.4ms      81.2     208KB     6.58    37     3      456ms
#> 2 st_filter      12ms 12.5ms      79.7     199KB     6.64    36     3      452ms
```

2. Inspect the full list of drivers, e.g. with the command `View(drvs)`. 
    - Which formats are you likely to use and why?
    - Bonus: take a look at [Chapter 8](https://r.geocompx.org/read-write) of Geocomputation with R for more on reading and writing geographic (including raster) data with R.
    
# Attribute operations with dplyr

**dplyr** is a large package with many functions for working with data frames. The five key 'verbs' [described](https://dplyr.tidyverse.org/) as:

> -   `mutate()` adds new variables that are functions of existing variables
> -   `select()` picks variables based on their names.
> -   `filter()` picks cases based on their values.
> -   `summarise()` reduces multiple values down to a single summary.
> -   `arrange()` changes the ordering of the rows.

Let's take a brief look at each.

```{r}
countries_modified = countries |>
  mutate(pop_density = pop / area_km2) |>
  select(name_long, pop_density) |>
  filter(pop_density > 100) |>
  arrange(desc(pop_density))
countries_modified
```

The `summarise()` function is often used in combination with `group_by()`, e.g. as follows:

```{r}
countries_summarised = countries |>
  group_by(contains_a = str_detect(name_long, "a")) |>
  summarise(n = n(), mean_pop = mean(pop))
countries_summarised
```

The operation creates a new variable called `contains_a` that is `TRUE` if the country name contains an "a" and `FALSE` otherwise. Perhaps more impressively, it also automatically updated the geometry column of the combined countries containing the letter "a", highlighting `dplyr`'s ability to work with geographic data represented as `sf` objects.

```{r}
countries_summarised |>
  ggplot() +
    geom_sf(aes(fill = contains_a)) +
    geom_sf(data = countries, fill = NA, linetype = 3) 
```

## Exercises

1.  Create a new data frame called `countries_modified2` that contains the name, population and area of countries with a population density of more than 100 people per km2, sorted by area in descending order.
2.  Do the same with base R functions and the `[` operator.
    -   What are the pros and cons of each?
    -   Which do you prefer?

```{r}
#| eval: false
#| echo: false
# with dplyr:
countries_modified2 = countries |>
  mutate(pop_density = pop / area_km2) |>
  select(name_long, pop_density, area_km2) |>
  filter(pop_density > 100) |>
  arrange(desc(area_km2))
# with base R:
countries_base = countries[countries$pop / countries$area_km2 > 100, ]
countries_base = countries_base[countries_base$area_km2 > 100, c("name_long", "pop", "area_km2")]
countries_base = countries_base[order(countries_base$area_km2, decreasing = TRUE), ]

waldo::compare(countries_modified2$name_long, countries_base$name_long)
```

# Making maps with ggplot2

As shown above, `geom_sf()` works 'out of the box' with geographic data.
We can modify plotting commands to control outputs and generate publishable maps.

```{r}
library(ggspatial)
countries |>
  ggplot() +
    geom_sf(fill = "grey80", color = "black") +
    geom_sf(data = countries_modified, aes(fill = pop_density)) +
    scale_fill_viridis_c() +
    theme_minimal()
```

Map making is an iterative and time consuming process.
Iterate on the code above, e.g. by changing the color palette, adding a title, and adding a legend.

There are many add-ons to `ggplot2` and one that is highly recommended is `ggspatial`, which can be used to add a basemap to a plot with `annotation_map_tile()`.

```{r}
#| message: false
rosm::osm.types()
ggplot() +
  annotation_map_tile() +
  layer_spatial(countries_modified, aes(fill = pop_density), linewidth = 3, alpha = 0.3) +
  scale_fill_viridis_c()

```

## Exercises

1. With reference to the documentation at [ggplot2.tidyverse.org/index.html](https://ggplot2.tidyverse.org/index.html), modify the code above to create a map with a title, legend and a different color palette.
2. With reference to [paleolimbot.github.io/ggspatial/](https://paleolimbot.github.io/ggspatial/), add annotations including scale bar, north arrow and a text label to the map.
3. Bonus: try map making with `tmap` and test out the interactive mode (set with `tmap_mode("interactive")`)."
4. Bonus: try reproducing maps presented in [Chapter 9](https://r.geocompx.org/adv-map.html) of Geocomputation with R with `ggplot2` and `ggspatial`. Which mapping framework do you prefer and why?
5. If you use raster data, take a look at the [`tidyterra`](https://dieghernan.github.io/tidyterra/) documentation.


# Worked example: data from Poznan

For many people 'learning by doing' is the most effective way to pick up new technical skills.
So let's work with some larger datasets.
Note: the input data and code below are adapted from Michael Dorman's tutorial on [Working with Spatial Data in Python](https://geobgu.xyz/presentations/p_2023_ogh).

## Getting and reading-in the data

```{bash}
#| eval: false
#| echo: false
# Zip the data folder:
zip -r data.zip data
gh release list
# Create release labelled data and upload the zip file:
gh release create data data.zip
```

To get the data for this session, download and unzip the [data.zip](https://github.com/Robinlovelace/opengeohub2023/releases/download/data/data.zip) file in the releases.
You can do that in R with the following commands:

```{r}
#| eval: false
u = "https://github.com/Robinlovelace/opengeohub2023/releases/download/data/data.zip"
f = basename(u)
if (!dir.exists("data")) {
  download.file(u, f)
  unzip(f)
}
```

Check you have downloaded the files with the following command:

```{r}
list.files("data")[1:3]
```

## Vector data

We'll start with a dataset that we'll create ourselves.

```{r}
#| eval: false
#| echo: false
fair_playce_poznan = stplanr::geo_code("FairPlayce, Poznan")
fair_playce_poznan

```

```{r}
```

```{r}
opengeo_df = tribble(
  ~name, ~lon, ~lat,
  "Faculty",        16.9418, 52.4643,
  "Hotel ForZa",    16.9474, 52.4436,
  "Hotel Lechicka", 16.9308, 52.4437,
  "FairPlayce",     16.9497, 52.4604
)
opengeo_sf = sf::st_as_sf(opengeo_df, coords = c("lon", "lat"))
sf::st_crs(opengeo_sf) = "EPSG:4326"
```

```{python}
#| echo: false
#| eval: false
# Python equivalent:
pnt1 = shapely.Point([16.9418, 52.4643]) #   'Faculty', 
pnt2 = shapely.Point([16.9474, 52.4436]) #    'Hotel ForZa', 
pnt3 = shapely.Point([16.9308, 52.4437]) #    'HL Hotel Lechicka'
pnt4 = shapely.Point([16.9497, 52.4604]) #    'FairPlayce Hotel'
```

```{r}
#| column: screen-inset-shaded

library(leaflet)
leaflet() %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addMarkers(
    lng = opengeo_df$lon,
    lat = opengeo_df$lat,
    popup = opengeo_df$name
  )
```

# `tidyverse` + `geos` workflows

The [`geos` package](https://paleolimbot.github.io/geos/) provides a simple and high-performance interface to the GEOS library for performing geometric operations on geographic data.
Load it as follows:

```{r}
library(geos)
```

`geos` works well with the `tidyverse` and `sf` packages, as shown below.

```{r}
countries_geos = as_geos_geometry(sf::st_geometry(countries))
countries_geos
```

The package only deals with geometries: the attribute data is removed when you convert an `sf` object to a `geos` object.
You can store `geos` objects in a data frame and still use `dplyr` functions to process them:

```{r}
countries_geos_df = bind_cols(countries_df, geos = countries_geos)
countries_summarised_df = countries_geos_df |>
  group_by(contains_a = str_detect(name_long, "a")) |>
  summarise(n = n(), mean_pop = mean(pop))
countries_summarised_df
```

Note: the `geos` column has gone!
This is because `geos` columns are not 'sticky' like `sf` columns.
Let's see how to get them back.

```{r}
#| layout-ncol: 2
countries_union1 = countries_geos |>
  geos_unary_union()
plot(countries_union1)
countries_union2 = countries_geos |>
  geos_make_collection() |>
  geos_unary_union()
plot(countries_union2)
```

However, you can add the union of the grouped columns as follows:

```{r}
countries_summarised_geos = countries_geos_df |>
  group_by(contains_a = str_detect(name_long, "a")) |>
  summarise(n = n(), mean_pop = mean(pop),
  geometry = geos_unary_union(geos_make_collection(geos)))
countries_summarised_geos
plot(countries_summarised_geos$geometry)
```

Convert back to an `sf` object as follows:

```{r}
countries_summarised_geos_sf = st_as_sf(countries_summarised_geos)
# waldo::compare(
#   countries_summarised,
#   countries_summarised_geos_sf
#   )
```

Aside from geometry names and minor differences in the geometries, the two objects are identical.
This raises the question: why use `geos` at all?
The answer can be found by following the exercises below.

## Exercises

1. Benchmark the union operation in `geos` and `sf` with the `bench::mark()` function.
    - Which is faster?
    - Which is easier to use?
    - Which do you prefer?

```{r}
#| eval: false
#| echo: false
bench::mark(check = FALSE,
  geos = countries_geos |>
    geos_make_collection() |>
    geos_unary_union(),
  sf = countries |>
    st_union()
)
```

# + friends

There are many packages that integrate with the `tidyverse`, or which are inspired by it.
Options include:

- [`tidyterra`](https://dieghernan.github.io/tidyterra/) for working with raster data
- [DuckDB](https://duckdb.org/docs/api/r.html) is a high-performance in-memory database that can be used with `dplyr` that is mature, well documented, and has a [spatial extension](https://duckdb.org/docs/extensions/spatial)
- [dtplyr](https://github.com/tidyverse/dtplyr/), a `dplyr`-compliant interface to [`data.table`](https://rdatatable.gitlab.io/data.table/) for working with large datasets
- The `collapse` package, which supports `tidyverse` syntax and which has some [integration with `sf`](https://sebkrantz.github.io/collapse/articles/collapse_and_sf.html)
- [`tidypolars`](https://github.com/etiennebacher/tidypolars), an R interface to the [`polars`](https://github.com/pola-rs/polars) Rust crate (note: there is also a popular [Python interface](https://pola-rs.github.io/polars-book/user-guide/installation/)
- [The geoarrow spec](https://github.com/geoarrow) and nascent implementations, none of which are yet ready for production use

How to assess which to use?
My advice would be to focus on solving the problem, rather than the tools, in which case the `tidyverse` + `sf` + `terra` or `stars` should be more than adequate for most purposes.
There can be major time savings made by sticking to established tools that are well documented and which have a large user base.

Developer time is more valuable than computer time, so a major factor in deciding which tool to use should be how reliable it is.
Time spent debugging issues can be a major time sink, especially for new packages that are not yet well tested or documented.
And for most purposes, the tools outlined in the previous sections are more than adequate, solving perhaps 90% of the problems you're likely to encounter.

There are times when performance is important but as the saying goes, premature optimisation is the root of all evil, so don't worry about performance until you have a working solution.

If you *do* need to wait longer than is feasible for your code to run, it is worth trying to identify the bottleneck (is there are a solution involving changing input datasets or workflows?) before having a look at alternatives.
Established benchmarks for *non-spatial data* are https://duckdblabs.github.io/db-benchmark/ and https://www.pola.rs/benchmarks.html .


This section is offered more as a place to experiment than any concrete advice.
My concrete advice is focus on the problem and solving it with the software is most appropriate for your use case.

## `tidypolars`

If you want to give this package a spin, run the following command:

```{r}
#| eval: false
install.packages(
  'tidypolars', 
  repos = c('https://etiennebacher.r-universe.dev/bin/linux/jammy/4.3', getOption("repos"))
)
```

## `rsgeo`

A work in progress is the [`rsgeo` package](https://github.com/JosiahParry/rsgeo), which aims to provide an seamless interface between R and the `geo` Rust crate.
This could open the possiblity of calling other high-performance Rust libraries from R, although the package is at an early stage of development and probably not ready for production use.

We can check the installation works as follows:

```{r}
#| eval: false
install.packages('rsgeo', repos = c('https://josiahparry.r-universe.dev', 'https://cloud.r-project.org'))
```

```{r}
library(rsgeo)
countries_rs  = as_rsgeom(sf::st_geometry(countries))
countries_rs
bench::mark(check = FALSE,
  sf = sf::st_union(countries),
  geos = geos::geos_make_collection(geos::geos_unary_union(countries_geos)),
  rsgeo = rsgeo::union_geoms(countries_rs)
)

```

# Further reading

The following free and open access resources provide a strong foundation for further learning in this space.

-   @wickham2023, hosted at <https://r4ds.hadley.nz/>, is a detailed introduction to the tidyverse

-   @lovelace2019 provides a wide ranging yet beginner-friendly introduction to using R for geographic analysis

-   @pebesma2023, a textbook on spatial data science by the developers of `sf` and other core 'rspatial' packages

-   Any other suggestions welcome, there's much more out there!